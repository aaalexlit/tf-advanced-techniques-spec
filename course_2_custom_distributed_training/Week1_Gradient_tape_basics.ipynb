{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPn7dOwu+4huvtmZJsQ9UHI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaalexlit/tf-advanced-techniques-spec/blob/main/course_2_custom_distributed_training/Week1_Gradient_tape_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "ZWmCJAc8TK6I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient tape"
      ],
      "metadata": {
        "id": "7mGR2c_hjGMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array([-1, 0, 1, 2, 3, 4], dtype=float)\n",
        "# y = 2*x - 1\n",
        "y_train = np.array([-3, -1, 1, 3, 5, 7], dtype=float)\n",
        "\n",
        "# Trainable variables\n",
        "w = tf.Variable(random.random(), trainable=True)\n",
        "b = tf.Variable(random.random(), trainable=True)\n",
        "\n",
        "# Loss function\n",
        "def simple_loss(real_y, pred_y):\n",
        "  return tf.abs(real_y - pred_y)\n",
        "\n",
        "# Learning rate\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "def fit_data(real_x, real_y):\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    # Make prediction\n",
        "    pred_y = w * real_x + b\n",
        "    # Calculate loss\n",
        "    reg_loss = simple_loss(real_y, pred_y)\n",
        "\n",
        "  # Calculate gradients\n",
        "  w_gradient = tape.gradient(reg_loss, w)\n",
        "  b_gradient = tape.gradient(reg_loss, b)\n",
        "\n",
        "  # Update vars\n",
        "  w.assign_sub(w_gradient * LEARNING_RATE)\n",
        "  b.assign_sub(b_gradient * LEARNING_RATE)\n",
        "\n",
        "for _ in range(500):\n",
        "  fit_data(x_train, y_train)\n",
        "\n",
        "print(f'y ~ {w.numpy()}x + {b.numpy()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zssl4YDhsaeF",
        "outputId": "bef8c588-f2bf-4297-e383-c7712dd024fd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y ~ 2.048419713973999x + -0.9715903401374817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple gradient"
      ],
      "metadata": {
        "id": "STsqJQldTC2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = tf.Variable([[1.0]])\n",
        "with tf.GradientTape() as tape:\n",
        "  loss = w * w\n",
        "\n",
        "tape.gradient(loss, w)"
      ],
      "metadata": {
        "id": "puZo7FqQwjYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d8801a-1909-42bf-83ab-34988e439c4e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.ones((2,2))\n",
        "print('x', x)\n",
        "with tf.GradientTape() as t:\n",
        "  t.watch(x)\n",
        "  y = tf.reduce_sum(x)\n",
        "  z = tf.square(y)\n",
        "\n",
        "# derivative of z wrt the original input tensor x\n",
        "dz_dx = t.gradient(z, x)\n",
        "print('dz_dx', dz_dx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8xWjRj6-oRY",
        "outputId": "08af2c0a-81f2-4975-e4b8-6daa38453c39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x tf.Tensor(\n",
            "[[1. 1.]\n",
            " [1. 1.]], shape=(2, 2), dtype=float32)\n",
            "dz_dx tf.Tensor(\n",
            "[[8. 8.]\n",
            " [8. 8.]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `persistent=True`"
      ],
      "metadata": {
        "id": "MXsHFBPqTa3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape(persistent=True) as t:\n",
        "\tt.watch(x)\n",
        "\ty = x * x\n",
        "\tz = y * y\n",
        "dz_dx = t.gradient(z, x)\n",
        "print(dz_dx)\n",
        "dy_dx = t.gradient(y, x)\n",
        "print(dy_dx)\n",
        "del t # Drop the reference to the tape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLao2C9V--Y_",
        "outputId": "b8ba18ee-483a-4135-fcb0-31112088fbe4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(108.0, shape=(), dtype=float32)\n",
            "tf.Tensor(6.0, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Higher-order gradients (Nested gradient tapes)\n",
        "\n"
      ],
      "metadata": {
        "id": "0NkyFPfMN5Em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(1.0)\n",
        "with tf.GradientTape() as tape_2:\n",
        "  with tf.GradientTape() as tape_1:\n",
        "    y = x * x * x\n",
        "  dy_dx = tape_1.gradient(y, x)\n",
        "d2y_dx2 = tape_2.gradient(dy_dx, x)\n",
        "\n",
        "assert dy_dx.numpy() == 3.0\n",
        "assert d2y_dx2.numpy() == 6.0"
      ],
      "metadata": {
        "id": "UlhWzjKoOmdF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Where not to indent the first gradient calculation\n",
        "If the first gradient calculation is OUTSIDE of the outer `with` block, it won't persist for the second gradient calculation."
      ],
      "metadata": {
        "id": "CdP_POpYUYNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(1.0)\n",
        "\n",
        "with tf.GradientTape() as tape_2:\n",
        "    with tf.GradientTape() as tape_1:\n",
        "        y = x * x * x\n",
        "\n",
        "# The first gradient call is outside the outer with block\n",
        "# so the tape will expire after this\n",
        "dy_dx = tape_1.gradient(y, x)\n",
        "\n",
        "# The tape is now expired and the gradient output will be `None`\n",
        "d2y_dx2 = tape_2.gradient(dy_dx, x)\n",
        "\n",
        "print(dy_dx)\n",
        "print(d2y_dx2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PRepfY_UZ5X",
        "outputId": "e7efe1c8-edf1-4da8-82f1-f8a38b3b8d26"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(3.0, shape=(), dtype=float32)\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}